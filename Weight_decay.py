"""æƒé‡è¡°å‡
è™½ç„¶å¢å¤§è®­ç»ƒæ•°æ®é›†å¯èƒ½ä¼šå‡è½»è¿‡æ‹Ÿåˆï¼Œ
ä½†æ˜¯è·å–é¢å¤–çš„è®­ç»ƒæ•°æ®å¾€å¾€ä»£ä»·é«˜æ˜‚ã€‚
åº”å¯¹è¿‡æ‹Ÿåˆé—®é¢˜çš„å¸¸ç”¨æ–¹æ³•ï¼šæƒé‡è¡°å‡ï¼ˆweight decay


æƒé‡è¡°å‡ç­‰ä»·äºL2èŒƒæ•°æ­£åˆ™åŒ–ï¼ˆregularizationï¼‰ã€‚
æ­£åˆ™åŒ–é€šè¿‡ä¸ºæ¨¡å‹æŸå¤±å‡½æ•°æ·»åŠ æƒ©ç½šé¡¹ä½¿å­¦å‡ºçš„æ¨¡å‹å‚æ•°å€¼è¾ƒå°ï¼Œæ˜¯åº”å¯¹è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ‰‹æ®µã€‚

L2èŒƒæ•°æ­£åˆ™åŒ–åœ¨æ¨¡å‹åŸæŸå¤±å‡½æ•°åŸºç¡€ä¸Šæ·»åŠ L2èŒƒæ•°æƒ©ç½šé¡¹
L2èŒƒæ•°æƒ©ç½šé¡¹æŒ‡çš„æ˜¯æ¨¡å‹æƒé‡å‚æ•°æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹å’Œä¸ä¸€ä¸ªæ­£çš„å¸¸æ•°çš„ä¹˜ç§¯
æ–°çš„æŸå¤±å‡½æ•°ï¼š â„“(w1,w2,b)+ï¼ˆÎ»/2nï¼‰*ï¼ˆ||w||**2ï¼‰,Î»>0 è¶…å‚æ•°   nä¸ºæ ·æœ¬æ•°

"""
###é«˜ç»´çº¿æ€§å®éªŒ###
import Myself_d2lzh as my
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import data as gdata, loss as gloss, nn

##è·å–æ•°æ®é›†##
n_train,n_test,num_inputs=20,100,200
true_w= nd.ones(shape=(num_inputs,1))*0.01
true_b =0.05
features = nd.random.normal(shape=(n_train+n_test,num_inputs))
labels = nd.dot(features,true_w)+true_b
labels +=nd.random.normal(scale=0.01,shape=labels.shape)
train_features, test_features = features[:n_train, :], features[n_train:, :]
train_labels, test_labels = labels[:n_train], labels[n_train:]

batch_size=1
num_epochs= 100
learning_rate= 0.003
net = my.linreg
loss = my.squared_loss
train_iter = gdata.DataLoader(gdata.ArrayDataset(train_features,train_labels),batch_size,shuffle=True)


def fit_and_plot(lanbd):
    w,b=init_params()
    train_ls,test_ls = [],[]
    for _ in range(num_epochs):
        for X,y in train_iter:
            with autograd.record():
                l=loss(net(X,w,b),y)+lanbd*l2_penalty(w)
            l.backward()
            my.sgd([w,b],learning_rate,batch_size)
        train_ls.append(loss(net(train_features,w,b),train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features, w, b), test_labels).mean().asscalar())
    my.semilogy(lanbd,range(1, num_epochs + 1), train_ls, 'epochs', 'loss',
                 range(1, num_epochs + 1), test_ls, ['train', 'test'])
    print('ğœ†= %d' % (lanbd),'   L2 norm of w:', w.norm().asscalar())
    #åˆå§‹åŒ–å‚æ•°#
def init_params():
    w = nd.random.normal(scale=1, shape=(num_inputs, 1))
    b = nd.zeros(shape=(1,))
    w.attach_grad()
    b.attach_grad()
    return [w, b]

##L2 èŒƒæ•°æƒ©ç½šé¡¹##
def l2_penalty(w):
    return (w**2).sum() / 2

fit_and_plot(0) #Î»=lanbd=0 è¡¨ç¤ºæ²¡ç”¨ä½¿ç”¨æƒé‡è¡°å‡
""" output:L2 norm of w: 13.155677 """

fit_and_plot(5) #Î»=lanbd=3 è¡¨ç¤ºä½¿ç”¨æƒé‡è¡°å‡
"""output: L2 norm of w: 0.042180628"""
fit_and_plot(15)
fit_and_plot(20)
fit_and_plot(50)

